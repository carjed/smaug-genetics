# Modeling effect of sequence context and genomic features

Our model to predict the effects of sequence context and genomic features requires a series of preprocessing steps to generate a set of 24,576 files (1 per each 7-mer subtype), each containing all positions in the genome within a particular sequence context, annotated as either a singleton (1) or non-mutated site (0).

## Preparing input data
This step is performed by calling the `R/build_logit_data.r` script from within the `R/mod_shell.r` script after loading and reshaping the list of singletons. `R/build_logit_data.r` calls `getNonMut.pl` to generate per-chromosome (22) and per-type (6) files located at `output/logmod_data/chr*_${type}_sites.txt`, then calls an `awk` function to split these files into per-chromosome (22), per-type (6), and per-motif (4096) subfiles, located at `/output/logmod_data/chr*/chr*_{type}_{motif}.txt`. Once completed, these per-chromosome files must be concatenated by type (6) x motif (4096) into: `output/logmod_data/motifs/{type}/{type}_{motif}.txt`

Each of these 24,576 files is then annotated with the mean depth of coverage across all individuals, using the .glf files generated by the variant calling pipeline. Processing these .glf files and annoting the data with per-site depth information involves running several additional scripts, detailed in `data_mgmt/per-site_dp/README.md`. The resulting files are generated into `output/logmod_data/motifs/{type}/dp/{type}_{motif}_dp.txt`.

## Running the models
Once data preparation is completed, we are ready to run the model on each subtype file. This is performed with the `R/log_mod.r` script, which implements logistic regression models with the `speedglm` package. This script writes out two files for each subtype: (1) the beta coefficients and standard errors for the features considered in the model, and (2) the predicted mutation rates for all sites of that subtype. Because we will eventually need these rates to be ordered by chromosome/postion, the predicted rates are split by chromosome into `output/predicted/{type}/chr*/{type}_{motif}.txt`.

The `R/logit_batch.txt` slurm batch file distributes these jobs across nodes for a given type. An additional helper script, `data_mgmt/logit_scripts/build_logit_batch.pl`, can be used to scan the slurm monitor for failed jobs, and generate secondary batch files to re-run them.

## Merging and sorting predicted rates
Because the predicted rates are written as per-chromsome, per-subtype files, we must concatenate all subtype files and sort by position for each chromsome. This is performed with the `data_mgmt/logit_scripts/slurm_sort_pred.txt` slurm batch file, which generates the chromosome (22) x type (6) files into `output/predicted/chr*.{type}.txt`.

## Extracting null background for validation models
These predicted rate files are the basis for generating the non-mutated background we use in our validation models on *de novo* mutations.

### Creating UCSC Genome Browser-compatible tracks
With an additional processing step, we can convert the predicted rate tables into BigWig (.bw) format for compatibility with the UCSC Genome Browser. This is performed using the `data_mgmt/toWig/toWig.sh` script (if desired, a slurm batch file to do this across all chromosomes is available at  `data_mgmt/toWig/toWig_batch.txt`).
