# GLF file processing

In our models used to estimate the effects of genomic features on mutability, it is crucial that we adjust for the depth-of-coverage at each site to minimize technical artifacts due to coverage biases. The scripts in this directory process the raw [genotype likelihood format (.glf)](http://genome.sph.umich.edu/wiki/GLF) files generated by our sequencing pipeline to extract the depth at each site, then annotate the model input data with this information.

### Dependencies
To run these scripts, we assume that the per-site input data files (generated by the `R/build_logit_data.R` script) exist in the appropriate directories (`output/logmod_data/motifs/{category}/{category}_{motif}.txt`), and there exists a file titled `output/glf_depth/glf_filelist.txt` containing the list of raw .glf files (with absolute path) to be processed. `glf_filelist.txt` can be generated with the following shell command: `find /path/to/glfs/samples/chr* -mindepth 1 -type f -name "*.glf" | grep -e "chr[0-9]" | grep -Fwf /path/to/sample/list/list.txt > output/glf_depth/glf_filelist.txt`, where `/path/to/glfs/samples/` is the parent directory containing the .glf files, and `/path/to/sample/list/list.txt` is a file containing the list of sample IDs (one per line).

These scripts require perl 5, version 18 or higher, [`samtools-hybrid`](https://github.com/statgen/samtools-0.1.7a-hybrid), and the [slurm](http://slurm.schedmd.com/slurm.html) workload management system be installed on your system. Note that the slurm batch files generated in these scripts contain parameters unique to our system environment; if you are using slurm, you will need to manually modify these parameters accordingly. If you do not have slurm, you will need to write custom versions of the batchfile-generating scripts to run the worker processes across the .glf files.

### Outline
This directory contains two sets of scripts:

1. Extracting per-site depth information from the raw .glf files (scripts start with "process_")
2. Annotating the data files with the mean depth (scripts start with "add_")

### Processing
Our variant calling pipeline generates one .glf file per individual per 5Mb chunk of sequence, resulting in tens of thousands of files that must be processed.

Processing of an individual .glf file is performed by the `process_glf_worker.pl` script, which calls `samtools-hybrid glfview` to generate a file containing the depth at each site. A second script, `process_glf_meandp.pl` further summarizes the

To more easily manage these thousands of jobs, the `process_glf_master.pl` script generates and submits slurm batch files to distribute the two worker processes across multiple nodes. Due to server limitations, the number of jobs requested in a batch file cannot exceed, . To handle this, we have set `process_glf_master.pl` to run a single chromosome at a time, and have wrapped this in a shell process, `process_glf_shell.pl` to loop through all chromosomes.

### Annotating
The `add_dp_worker.pl` script is responsible for adding a depth column to the each of the input data files (one per 7-mer subtype, or 4,096 x 6 = 24,576 total subtypes). The `add_dp_master.pl` script generates and submits a batch file to perform the worker process on each subtype file; this master script must be run once per mutation category, specified with the `-categ` argument.

Output files containing the depth at each site are written to `output/logmod_data/motifs/dp/{category}/{category}_{motif}_dp.txt`. These files are then ready to be passed to the `log_mod.r` script to run the models.
